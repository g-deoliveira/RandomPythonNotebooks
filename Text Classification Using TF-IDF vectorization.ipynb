{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification Using TF-IDF\n",
    "The dataset consists of 2225 documents from the BBC news website corresponding to stories in five topical areas from 2004-2005.\n",
    "\n",
    "* business\n",
    "* entertainment\n",
    "* politics\n",
    "* sport\n",
    "* tech\n",
    "\n",
    "Downloaded from here: http://mlg.ucd.ie/datasets/bbc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.externals import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/Users/guilhermede-oliveira/Library/DataScienceStudio/dss_design_v5/managed_folders/BBCARTICLES/hZFXEdPS/BBC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/guilhermede-oliveira/Library/DataScienceStudio/dss_design_v5/managed_folders/BBCARTICLES/hZFXEdPS/BBC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-48f9fa4c1e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0marticle_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfolder_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticle_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/guilhermede-oliveira/Library/DataScienceStudio/dss_design_v5/managed_folders/BBCARTICLES/hZFXEdPS/BBC'"
     ]
    }
   ],
   "source": [
    "# load articles into a Pandas DataFrame\n",
    "\n",
    "#path = '/Users/guilhermede-oliveira/Library/DataScienceStudio/dss_design_v5/managed_folders/BBCARTICLES/hZFXEdPS/BBC'\n",
    "\n",
    "results = []\n",
    "for article_class in os.listdir(root_path):\n",
    "    folder_path = os.path.join(root_path, article_class)\n",
    "    print(folder_path)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    else:\n",
    "        #print article_class, path\n",
    "        articles = os.listdir(folder_path)\n",
    "        print(len(articles))\n",
    "        for article in sorted(articles):\n",
    "            #print article\n",
    "            article_path = os.path.join(folder_path, article)\n",
    "            #print article_path\n",
    "            with open(article_path, 'r') as f:\n",
    "                text = f.readlines()\n",
    "            #print text\n",
    "            results.append([article_class, article, text])\n",
    "\n",
    "df = pd.DataFrame(results, columns=['label', 'file_name', 'raw_text'])\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225\n"
     ]
    }
   ],
   "source": [
    "# load each article into a variable X, perform some preprocessing, ie\n",
    "# remove line-breaks ('\\n')\n",
    "# this is an area where a lot of additional preprocessing could be added\n",
    "# for instance you could remove numbers, extra spaces, etc...\n",
    "X = df['raw_text'].values\n",
    "X = [' '.join(x).replace('\\n', '') for x in X]\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'politics': 3, 'sport': 2, 'tech': 4, 'business': 1, 'entertainment': 0}\n"
     ]
    }
   ],
   "source": [
    "# encode the labels into the y-variable\n",
    "labels = df['label'].unique().tolist()\n",
    "labels = {label:k for k,label in enumerate(labels)}\n",
    "print(labels)\n",
    "y = df['label'].map(labels).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1852)\n"
     ]
    }
   ],
   "source": [
    "# in the TF-IDF below, we are getting rid of english stop words, we are considering unigrams and bigrams, \n",
    "# we are pruning the dictionary (max_df, min_df), and we are ignoring decoding errors\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.8, min_df=40, decode_error='ignore')\n",
    "\n",
    "X_vectorized = vectorizer.fit_transform(X)\n",
    "print(X_vectorized.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8dabec42dac9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# these are the first 100 tokens in the vectorized documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# these are the first 100 tokens in the vectorized documents\n",
    "print(vectorizer.get_feature_names()[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## Train/Validation Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2225, 1852)\n",
      "(1891, 1852) (1891,)\n",
      "(334, 1852) (334,)\n"
     ]
    }
   ],
   "source": [
    "# split the data into a training/test set and a validation set\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_vectorized, y, test_size=0.15, random_state=42)\n",
    "\n",
    "print(X_vectorized.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Forest Classifier Using Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
      "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False),\n",
      "       fit_params=None, iid='warn', n_jobs=None,\n",
      "       param_grid={'n_estimators': [100], 'min_samples_split': [10, 20, 40], 'criterion': ['gini', 'entropy'], 'max_depth': [3, 6, 9, 12, 15], 'min_samples_leaf': [10, 20, 40]},\n",
      "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
      "       scoring=None, verbose=0)\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "params = {\"n_estimators\":[100],\n",
    "          \"max_depth\": [3, 6, 9, 12, 15],\n",
    "          \"min_samples_split\": [10, 20, 40],\n",
    "          \"criterion\": [\"gini\", \"entropy\"],\n",
    "          \"min_samples_leaf\": [10, 20, 40]\n",
    "         }\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=params, cv=3)\n",
    "print(grid_search)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_estimators': [100], 'min_samples_split': [10, 20, 40], 'criterion': ['gini', 'entropy'], 'max_depth': [3, 6, 9, 12, 15], 'min_samples_leaf': [10, 20, 40]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_vectorized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_split': 20, 'n_estimators': 100, 'criterion': 'entropy', 'max_depth': 12, 'min_samples_leaf': 10}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917752808988764\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance on the Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the model to the validation set\n",
    "\n",
    "y_pred = grid_search.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334,) (334,)\n",
      "321\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape, y_val.shape)\n",
    "print(sum(y_pred == y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550898203592815"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this means out of 334 records in the validation set, 319 were predicted correctly\n",
    "319/334."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pickled model and model parameters to Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guilhermede-oliveira/Documents/Columbia/Fall 2018 - Capstone/Text Classification/random_forest_classifier.pkl\n",
      "/Users/guilhermede-oliveira/Documents/Columbia/Fall 2018 - Capstone/Text Classification/model_parameters.json\n"
     ]
    }
   ],
   "source": [
    "model_path = '/Users/guilhermede-oliveira/Documents/Columbia/Fall 2018 - Capstone/Text Classification'\n",
    "\n",
    "pickle_file_path = os.path.join(model_path, 'random_forest_classifier.pkl')\n",
    "param_file_path = os.path.join(model_path, 'model_parameters.json')\n",
    "\n",
    "print(pickle_file_path)\n",
    "print(param_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = grid_search.best_estimator_\n",
    "model_parameters = predictor.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/guilhermede-oliveira/Documents/Columbia/Fall 2018 - Capstone/Text Classification/random_forest_classifier.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write pickled model to folder\n",
    "joblib.dump(predictor, pickle_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model paramters to folder\n",
    "with open(param_file_path, 'w') as fp:\n",
    "    json.dump(model_parameters, fp, sort_keys=True, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00129982, 0.        , 0.0003993 , ..., 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(predictor.feature_importances_,\n",
    "                  index = vectorizer.get_feature_names(),\n",
    "                  columns=['importance']).sort_values('importance',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1852, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mr</th>\n",
       "      <td>mr</td>\n",
       "      <td>0.036499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>market</td>\n",
       "      <td>0.028711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>government</td>\n",
       "      <td>0.028004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <td>game</td>\n",
       "      <td>0.024744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>users</th>\n",
       "      <td>users</td>\n",
       "      <td>0.022529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature  importance\n",
       "mr                  mr    0.036499\n",
       "market          market    0.028711\n",
       "government  government    0.028004\n",
       "game              game    0.024744\n",
       "users            users    0.022529"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi['feature'] = fi.index\n",
    "fi = fi[['feature', 'importance']]\n",
    "print(fi.shape)\n",
    "\n",
    "fi.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells us the tokens in the text that had the most important effect on the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nautiluslabs",
   "language": "python",
   "name": ".nautiluslabs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
